import streamlit as st
from langchain_ollama import OllamaLLM
from langchain_community.tools import DuckDuckGoSearchRun
from opencc import OpenCC

# ==========================================
# 1. æ ¸å¿ƒåˆå§‹åŒ–
# ==========================================
cc = OpenCC('s2twp')
search = DuckDuckGoSearchRun()

# ç¶²é åŸºæœ¬è¨­å®š (æ¨™é¡Œæ”¹ç‚ºè¼ƒä¸­æ€§çš„ç³»çµ±åç¨±)
st.set_page_config(page_title="Terminal | System 12B", layout="wide")

# CSS å…¨åŸŸç½®ä¸­èˆ‡ç¾åŒ–æ³¨å…¥
st.markdown("""
    <style>
    /* è®“æ‰€æœ‰æ–‡å­—èˆ‡å°è©±æ¡†ç½®ä¸­ */
    .stApp h1, .stApp h2, .stApp h3, .stApp p {
        text-align: center !important;
    }
    .stChatMessage {
        display: flex !important;
        justify-content: center !important;
    }
    .stChatMessageContent {
        max-width: 800px !important;
        text-align: left !important; 
        margin: 0 auto !important;
    }
    /* éš±è—ä¸Šæ–¹è£é£¾æ¢ */
    header {visibility: hidden;}
    /* è®“ spinner ç½®ä¸­ */
    .stSpinner {
        display: flex;
        justify-content: center;
    }
    </style>
    """, unsafe_allow_html=True)

# ==========================================
# 2. å´é‚Šæ¬„ (System Settings)
# ==========================================
with st.sidebar:
    st.markdown("### ğŸ› ï¸ SYSTEM CONTROL")
    st.divider()
    internet_on = st.toggle("ğŸŒ å…¨çƒé€£ç¶²æ¨¡å¼", value=True)
    st.divider()
    st.subheader("Hardware Status")
    st.code("GPU: RTX 4060 8GB\nCore: Mistral NeMo 12B\nType: Local Edge Computing")
    
    if st.button("ğŸ—‘ï¸ CLEAR MEMORY", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ==========================================
# 3. åŠ è¼‰ Ollama æ¨¡å‹
# ==========================================
@st.cache_resource
def load_llm():
    return OllamaLLM(model="mistral-nemo")

try:
    llm = load_llm()
except Exception:
    st.error("âŒ æ ¸å¿ƒå¼•æ“æœªå•Ÿå‹•ï¼Œè«‹ç¢ºèª Ollama æ˜¯å¦åŸ·è¡Œä¸­")

# ==========================================
# 4. æˆæ¬Šé– (æ¥µç°¡ç™»å…¥ä»‹é¢)
# ==========================================
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.markdown("<br><br><br>", unsafe_allow_html=True)
    st.markdown("<h2 style='font-weight: 200; letter-spacing: 5px;'>SYSTEM ENCRYPTION</h2>", unsafe_allow_html=True)
    st.markdown("<p style='color: #888;'>æ­¤é€£ç·šå—ç«¯å°ç«¯åŠ å¯†ä¿è­·ï¼Œè«‹è¼¸å…¥é©—è­‰é‡‘é‘°</p>", unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1,1,1])
    with col2:
        auth_code = st.text_input("Access Key", type="password", label_visibility="collapsed")
        if st.button("ENTER", use_container_width=True):
            if auth_code == "12345":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("Invalid Key.")
    st.stop()

# ==========================================
# 5. ä¸»å°è©±ä»‹é¢
# ==========================================
# æ¥µç°¡é ‚éƒ¨æ¨™ç¤º
st.markdown("<h3 style='font-weight: 300; color: #444;'>TERMINAL_LOG_v1.0</h3>", unsafe_allow_html=True)
st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ¸²æŸ“æ­·å²å°è©±
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è™•ç†è¼¸å…¥æŒ‡ä»¤
if prompt := st.chat_input("Waiting for instruction..."):
    # ç´€éŒ„ä½¿ç”¨è€…å•é¡Œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI å›æ‡‰éƒ¨åˆ†
    with st.chat_message("assistant"):
        response_container = st.empty()
        
        try:
            with st.spinner("ğŸ“¡ æ­£åœ¨æ“·å–è¡›æ˜Ÿæ•¸æ“šä¸¦é€²è¡Œ 12B é‹ç®—..."):
                context = ""
                # å¦‚æœé–‹å•Ÿè¯ç¶²åŠŸèƒ½
                if internet_on:
                    search_result = search.run(f"latest news about {prompt}")
                    context = f"\nã€å³æ™‚åƒè€ƒè³‡è¨Šã€‘ï¼š{search_result}\n"

                # ç³»çµ± Prompt è¨­å®š
                system_prompt = (
                    "ä½ æ˜¯ç”± yangyanmao0707 é–‹ç™¼çš„å°ˆæ¥­ AI åŠ©æ‰‹ã€‚\n"
                    "1. å¿…é ˆå®Œå…¨ä½¿ç”¨è‡ºç£ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\n"
                    "2. åš´ç¦ä½¿ç”¨å¤§é™¸ç”¨èªï¼ˆä¾‹å¦‚ï¼šè¦–é »ã€è»Ÿä»¶ã€æ‰“å°ï¼‰ã€‚\n"
                    "3. èªæ°£ä¿æŒå°ˆæ¥­ã€ç°¡æ½”ã€ç§‘å­¸åŒ–ã€‚"
                )
                
                # åŸ·è¡Œä¸²æµè¼¸å‡º
                full_response = ""
                input_query = f"{system_prompt}\n{context}\nUser Instruction: {prompt}"
                
                for chunk in llm.stream(input_query):
                    # ç°¡é«”è½‰ç¹é«”
                    converted_chunk = cc.convert(chunk)
                    full_response += converted_chunk
                    # å³æ™‚é¡¯ç¤ºå‹•æ…‹æ‰“å­—æ•ˆæœ
                    response_container.markdown(full_response + "â–Œ")
                
                # æœ€çµ‚å®Œæˆç‰ˆæœ¬å»é™¤æ¸¸æ¨™
                response_container.markdown(full_response)
                # å„²å­˜å°è©±ç´€éŒ„
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"ğŸ›°ï¸ é€£ç·šç•°å¸¸ï¼š{str(e)}")
