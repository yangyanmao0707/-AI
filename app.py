import streamlit as st
from langchain_ollama import OllamaLLM
from langchain_community.tools import DuckDuckGoSearchRun
from opencc import OpenCC

# ==========================================
# 1. æ ¸å¿ƒåˆå§‹åŒ– (ç¶­æŒåŸæ¨£)
# ==========================================
cc = OpenCC('s2twp')
search = DuckDuckGoSearchRun()

# ä¿®æ”¹ç¶²é åˆ†é æ¨™é¡Œ
st.set_page_config(page_title="å¤ªç©º AI | 12B ç ”ç©¶çµ‚ç«¯", layout="wide")

st.markdown("""
    <style>
    .stApp h1, .stApp h2, .stApp h3, .stApp p { text-align: center !important; }
    .stChatMessage { display: flex !important; justify-content: center !important; }
    .stChatMessageContent { max-width: 800px !important; text-align: left !important; margin: 0 auto !important; }
    header {visibility: hidden;}
    .stSpinner { display: flex; justify-content: center; }
    </style>
    """, unsafe_allow_html=True)

# ==========================================
# 2. å´é‚Šæ¬„ (System Settings)
# ==========================================
with st.sidebar:
    st.markdown("### ğŸ›°ï¸ å¤ªç©º AI æ§åˆ¶ä¸­å¿ƒ")  # æ”¹ç‚ºå¤ªç©º AI
    st.divider()
    internet_on = st.toggle("ğŸŒ å…¨çƒé€£ç¶²æ¨¡å¼", value=True)
    st.divider()
    st.subheader("ç¡¬é«”å‹•åŠ›ä¾†æº")
    st.code("GPU: RTX 4060 8GB\nCore: Mistral NeMo 12B\nType: Local Edge Computing")
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºè¨˜æ†¶é«”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ==========================================
# 3. åŠ è¼‰ Ollama æ¨¡å‹ (ç¶­æŒåŸæ¨£)
# ==========================================
@st.cache_resource
def load_llm():
    return OllamaLLM(model="mistral-nemo")

try:
    llm = load_llm()
except Exception:
    st.error("âŒ æ¨¡å‹å¼•æ“æœªå•Ÿå‹•ï¼Œè«‹ç¢ºèª Ollama æ˜¯å¦åŸ·è¡Œä¸­")

# ==========================================
# 4. æˆæ¬Šé– (æ¥µç°¡ç™»å…¥ä»‹é¢)
# ==========================================
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.markdown("<br><br><br>", unsafe_allow_html=True)
    st.markdown("<h2 style='font-weight: 300; letter-spacing: 8px;'>å¤ªç©º AI ç ”ç©¶çµ‚ç«¯</h2>", unsafe_allow_html=True) # æ”¹ç‚ºå¤ªç©º AI
    st.markdown("<p style='color: #888;'>æ­¤é€£ç·šå—ç«¯å°ç«¯åŠ å¯†ä¿è­·ï¼Œè«‹è¼¸å…¥é©—è­‰é‡‘é‘°</p>", unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1,1,1])
    with col2:
        auth_code = st.text_input("Access Key", type="password", label_visibility="collapsed")
        if st.button("é©—è­‰é€²å…¥", use_container_width=True):
            if auth_code == "12345":
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("é‡‘é‘°ç„¡æ•ˆ")
    st.stop()

# ==========================================
# 5. ä¸»å°è©±ä»‹é¢
# ==========================================
st.markdown("<h1 style='color: #0056b3; letter-spacing: 5px;'>å¤ªç©º AI ç ”ç©¶ç³»çµ±</h1>", unsafe_allow_html=True) # æ”¹å›è—è‰²å¤§æ¨™é¡Œ
st.divider()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥æŒ‡ä»¤..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        response_container = st.empty()
        
        try:
            with st.spinner("ğŸ›°ï¸ æ­£åœ¨æ“·å–è¡›æ˜Ÿæ•¸æ“šä¸¦é€²è¡Œ 12B é‹ç®—..."):
                context = ""
                if internet_on:
                    # èª¿æ•´æœå°‹é—œéµå­—ä»¥ç²å¾—æ›´ä½³æ•ˆæœ
                    search_result = search.run(f"å¤ªç©º èˆªå¤ª æœ€æ–°è³‡è¨Š {prompt}")
                    context = f"\nã€å³æ™‚åƒè€ƒè³‡è¨Šã€‘ï¼š{search_result}\n"

                # ç³»çµ± Prompt è¨­å®š
                system_prompt = (
                    "ä½ æ˜¯ç”± yangyanmao0707 é–‹ç™¼çš„ã€å¤ªç©º AIã€ã€‚\n"
                    "1. å¿…é ˆå®Œå…¨ä½¿ç”¨è‡ºç£ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\n"
                    "2. åš´ç¦ä½¿ç”¨å¤§é™¸ç”¨èªï¼ˆä¾‹å¦‚ï¼šè¦–é »ã€è»Ÿä»¶ã€æ‰“å°ï¼‰ã€‚\n"
                    "3. èªæ°£ä¿æŒå°ˆæ¥­ã€ç°¡æ½”ã€ç§‘å­¸åŒ–ã€‚"
                )
                
                full_response = ""
                input_query = f"{system_prompt}\n{context}\nä½¿ç”¨è€…æŒ‡ä»¤ï¼š{prompt}"
                
                for chunk in llm.stream(input_query):
                    converted_chunk = cc.convert(chunk)
                    full_response += converted_chunk
                    response_container.markdown(full_response + "â–Œ")
                
                response_container.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"ğŸ›°ï¸ é€£ç·šç•°å¸¸ï¼š{str(e)}")
