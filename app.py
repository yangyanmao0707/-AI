import streamlit as st
from langchain_ollama import OllamaLLM
from langchain_community.tools import DuckDuckGoSearchRun # æ–°åŠŸèƒ½ï¼šè¯ç¶²æœå°‹
from opencc import OpenCC

# åˆå§‹åŒ–è½‰æ›å™¨èˆ‡æœå°‹å·¥å…·
cc = OpenCC('s2twp')
search = DuckDuckGoSearchRun()

# ==========================================
# 1. ç¶²é åŸºæœ¬è¨­å®š & CSS å…¨åŸŸç½®ä¸­æ³¨å…¥ (å®Œå…¨ç¶­æŒåŸç‰ˆ)
# ==========================================
st.set_page_config(page_title="å¤ªç©º AI | 12B å…¨é€Ÿçµ‚ç«¯", layout="wide")

st.markdown("""
    <style>
    /* 1. è®“ä¸»æ¨™é¡Œèˆ‡æ‰€æœ‰ Markdown æ–‡å­—ç½®ä¸­ */
    .stApp h1, .stApp h2, .stApp h3, .stApp p {
        text-align: center !important;
    }
    
    /* 2. è®“å°è©±æ°£æ³¡å®¹å™¨ç½®ä¸­ */
    .stChatMessage {
        display: flex !important;
        justify-content: center !important;
        text-align: center !important;
    }
    
    /* 3. é™åˆ¶å°è©±å…§å®¹å¯¬åº¦ä¸¦ç¶­æŒç½®ä¸­ */
    .stChatMessageContent {
        max-width: 800px !important;
        text-align: left !important; 
        margin: 0 auto !important;
    }

    /* 4. è®“ spinner è¼‰å…¥åœ–ç¤ºç½®ä¸­ */
    .stSpinner {
        display: flex;
        justify-content: center;
    }
    </style>
    """, unsafe_allow_html=True)

# ==========================================
# 2. å´é‚Šæ¬„ (ç¶­æŒåŸæ¨£ï¼Œåƒ…å¢åŠ è¯ç¶²é–‹é—œ)
# ==========================================
with st.sidebar:
    st.markdown("# ğŸ›°ï¸ ä»»å‹™æ§åˆ¶ä¸­å¿ƒ")
    st.divider()
    internet_on = st.toggle("ğŸŒ é–‹å•Ÿè¯ç¶²æ¨¡å¼", value=True) # æ–°å¢é–‹é—œ
    st.divider()
    st.subheader("ç¡¬é«”å‹•åŠ›ä¾†æº")
    st.code("GPU: RTX 4060 8GB\nModel: Mistral NeMo")
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºè¨˜æ†¶é«”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# ==========================================
# 3. åˆå§‹åŒ–æ¨¡å‹
# ==========================================
@st.cache_resource
def load_llm():
    return OllamaLLM(model="mistral-nemo")

try:
    llm = load_llm()
except Exception:
    st.error("âŒ æ¨¡å‹å¼•æ“æœªå•Ÿå‹•")

# ==========================================
# 4. æˆæ¬Šé– (ç¶­æŒåŸæ¨£)
# ==========================================
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.markdown("##  å¤ªç©º AI ç ”ç©¶çµ‚ç«¯")
    col1, col2, col3 = st.columns([1,1,1])
    with col2:
        auth_code = st.text_input("ğŸ”‘ è¼¸å…¥é©—è­‰ç¢¼", type="password")
        if st.button("é©—è­‰é€²å…¥", use_container_width=True):
            if auth_code == "12345":
                st.session_state.authenticated = True
                st.rerun()
    st.stop()

# ==========================================
# 5. ä¸»å°è©±ä»‹é¢ (ç¶­æŒåŸæ¨£)
# ==========================================
st.markdown("<h1 style='color: #0056b3;'> å¤ªç©º AI ç ”ç©¶ç³»çµ±</h1>", unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è™•ç†è¼¸å…¥
if prompt := st.chat_input("è«‹è¼¸å…¥èˆªå¤ªæŒ‡ä»¤..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
        
    with st.chat_message("assistant"):
        response_container = st.empty() # ç”¨æ–¼å‹•æ…‹ä¸²æµæ›´æ–°
        
        try:
            with st.spinner("ğŸ›°ï¸ è¡›æ˜Ÿé€šè¨Šè¨ˆç®—ä¸­..."):
                context = ""
                # åŠŸèƒ½ 1ï¼šè¯ç¶²æœå°‹ (å¦‚æœé–‹é—œæ‰“é–‹)
                if internet_on:
                    search_result = search.run(f"å¤ªç©º èˆªå¤ª æœ€æ–° {prompt}")
                    context = f"\nã€å³æ™‚è³‡è¨Šã€‘ï¼š{search_result}\n"

                system_prompt = (
                    "ä½ æ˜¯ç”±å°ç£é–‹ç™¼çš„ã€å¤ªç©º AIã€ã€‚è«‹éµå¾ªï¼š\n"
                    "1. å¿…é ˆä½¿ç”¨è‡ºç£ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚\n"
                    "2. åš´ç¦ç”¨å¤§é™¸ç”¨èªã€‚"
                )
                
                # åŠŸèƒ½ 2ï¼šä¸²æµè¼¸å‡º (è®“æ–‡å­—é€å­—å‡ºç¾)
                full_response = ""
                input_query = f"{system_prompt}\n{context}\næŒ‡ä»¤ï¼š{prompt}"
                
                for chunk in llm.stream(input_query):
                    converted_chunk = cc.convert(chunk)
                    full_response += converted_chunk
                    # åœ¨å®¹å™¨ä¸­å³æ™‚æ¸²æŸ“
                    response_container.markdown(full_response + "â–Œ")
                
                # æœ€çµ‚å®Œæˆç‰ˆæœ¬
                response_container.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"ğŸ›°ï¸ é€šè¨Šç•°å¸¸ï¼š{e}")